{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1582415",
   "metadata": {},
   "source": [
    "## 5. 옵티마이저\n",
    "- 하이퍼파라미터 중 하나\n",
    "- 모델 훈련 전 설정 단계인 메서드 `compile()`에서 손실함수를 지정해주면서 여러 *경사 하강법 알고리즘* 중 하나를 선택 가능하게 해주는 매개변수\n",
    "- 케라스의 기본 경사하강법 알고리즘: **RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588071a",
   "metadata": {},
   "source": [
    "**가장 기본적인 옵티마이저: 확률적 경사 하강법 (SGD)**\n",
    "- 이름은 SGD지만 기본적으로 미니배치를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1cd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'sgd'\n",
    "              , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = 'accuracy')\n",
    "# tensorflow.keras.optimizers 패키지 아래에 구현된 SGD 클래스와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "723c0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD() # keras.optimizers.SGD() 객체 정의\n",
    "    # learning_rate 매개변수 지정 가능\n",
    "\n",
    "model.compile(optimizer = sgd\n",
    "              , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7966f",
   "metadata": {},
   "source": [
    "### 5-1. 기본 경사 하강법 옵티마이저 (SGD 클래스에서 모두 제공)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567f851",
   "metadata": {},
   "source": [
    "##### learning_rate (학습률)\n",
    "- `sgd = keras.optimizers.SGD(learning_rate = 0.1)`\n",
    "- 경사하강법 알고리즘은 기울기에 학습률 (learning rate) 또는 보폭 (step size)라고 불리는 스칼라를 곱해 다음 지점을 결정짓는다.\n",
    "- local minimum에 효율적으로 도달할 수 있도록 너무 크지도 작지도 않은 적절한 학습률을 지정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ddb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebad88b",
   "metadata": {},
   "source": [
    "##### momentum > 0 (모멘텀)\n",
    "- `sgd = keras.optimizers.SGD(momentum = 0.9`\n",
    "- SGD 클래스의 momentum 매개변수의 기본값은 0\n",
    "- 0보다 큰 값으로 지정하면 이전의 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용\n",
    "- 보통 momentum 매개변수는 0.9 이상을 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f189c",
   "metadata": {},
   "source": [
    "##### nesterov = True (네스테로프 모멘텀)\n",
    "- `sgd = keras.optimizers.SGD(nesterov = True)`\n",
    "- 네스테로프 모멘텀 최적화 사용\n",
    "- 모멘텀 최적화를 2번 반복하여 구현\n",
    "- 기본 SGD보다 더 나은 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbbc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d019d24",
   "metadata": {},
   "source": [
    "### 5-2. 적응적 학습률 옵티마이저\n",
    "- 모델이 최적점에 가까이 갈수록 학습률(보폭)을 낮추어 안정적으로 최적점에 수렴하게 해주는 학습률\n",
    "- 학습률 매개변수를 튜닝하는 수고를 덜 수 있다는 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87fabe",
   "metadata": {},
   "source": [
    "##### RMSprop (옵티마이저 기본값)\n",
    "- `rmsprop = keras.optimizers.RMSprop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d2c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer = rmsprop\n",
    "             , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bb18e",
   "metadata": {},
   "source": [
    "##### Adagrad\n",
    "- `adagrad = keras.optimizers.Adagrad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc83cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = keras.optimizers.Adagrad()\n",
    "model.compile(optimizer = adagrad\n",
    "             , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576df236",
   "metadata": {},
   "source": [
    "### 5-3. Adam optimizers\n",
    "- 모멘텀 최적화와 RMSprop 장점을 접목한 옵티마이저\n",
    "- 널리 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba3caf",
   "metadata": {},
   "source": [
    "## 6. Adam optimizers를 이용한 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd62138",
   "metadata": {},
   "source": [
    "### 6-1. 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6accb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = (28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518ef72",
   "metadata": {},
   "source": [
    "### 6-2. 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7788ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam'\n",
    "             , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302de1c7",
   "metadata": {},
   "source": [
    "### 6-3. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d77eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 1s 822us/step - loss: 0.5275 - accuracy: 0.8149\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 1s 811us/step - loss: 0.3992 - accuracy: 0.8571\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 1s 810us/step - loss: 0.3558 - accuracy: 0.8712\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 1s 815us/step - loss: 0.3295 - accuracy: 0.8797\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 1s 818us/step - loss: 0.3069 - accuracy: 0.8871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feeed8ad600>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_scaled, train_target, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8aee25",
   "metadata": {},
   "source": [
    "### 6-4. 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab55684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 619us/step - loss: 0.3568 - accuracy: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3568277060985565, 0.8694166541099548]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039322d",
   "metadata": {},
   "source": [
    "- **train set, test set 각각 소폭 정확성이 하락했으나, 통상적으로 RMSporp 보다 조금 나은 성능을 낸다고 알려져있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddaa67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
